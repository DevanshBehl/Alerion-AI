<![CDATA[<div align="center">

# üì° Alerion AI

### Distributed Edge‚ÄìFog AI Monitoring Platform

**Real-time industrial anomaly detection** powered by Kafka streaming, ML inference, and WebSocket-driven dashboards.

[![Node.js](https://img.shields.io/badge/Node.js-22-339933?logo=node.js&logoColor=white)](https://nodejs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178C6?logo=typescript&logoColor=white)](https://typescriptlang.org/)
[![Apache Kafka](https://img.shields.io/badge/Kafka-KRaft_Mode-231F20?logo=apachekafka&logoColor=white)](https://kafka.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.11-3776AB?logo=python&logoColor=white)](https://python.org/)
[![React](https://img.shields.io/badge/React-19-61DAFB?logo=react&logoColor=black)](https://react.dev/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)](https://docker.com/)

</div>

---

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
  - [High-Level Diagram](#high-level-diagram)
  - [Data Flow Pipeline](#data-flow-pipeline)
  - [Component Interaction](#component-interaction)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Getting Started](#getting-started)
  - [1. Clone the Repository](#1-clone-the-repository)
  - [2. Start the Fog Layer (Docker)](#2-start-the-fog-layer-docker)
  - [3. Start Edge Nodes](#3-start-edge-nodes)
  - [4. Start the Dashboard](#4-start-the-dashboard)
- [Running Modes](#running-modes)
  - [Single Machine (Development)](#single-machine-development)
  - [Multi-Laptop (Distributed Demo)](#multi-laptop-distributed-demo)
- [ML Model Details](#ml-model-details)
- [API Reference](#api-reference)
- [Kafka Topics](#kafka-topics)
- [Environment Variables](#environment-variables)
- [Scalability & Performance](#scalability--performance)
- [Future Roadmap](#future-roadmap)
- [Contributing](#contributing)
- [License](#license)

---

## Overview

**Alerion AI** simulates a full-scale industrial IoT monitoring pipeline. It generates machine sensor telemetry from edge nodes, streams it through Apache Kafka to a fog layer that performs both statistical boundary checks and ML-based anomaly detection, and pushes real-time predictions to a React dashboard via WebSockets.

> This project demonstrates scalable distributed systems architecture in a local multi-device environment.

**Key capabilities:**
- üè≠ **5 independent edge nodes** simulating CNC machines with realistic sensor drift & anomaly spikes
- ‚ö° **Apache Kafka (KRaft mode)** for fault-tolerant, high-throughput event streaming
- ü§ñ **Dual ML inference** ‚Äî Flask standalone API + Dockerized FastAPI Kafka consumer
- üìä **Real-time React dashboard** with live charts, alert feeds, and machine health views
- üê≥ **One-command Docker deployment** orchestrating Kafka, ML, and backend

---

## Architecture

### High-Level Diagram

```mermaid
graph TB
    subgraph Edge["üîß Edge Layer"]
        E1["Edge Node 1<br/>(CNC Mill ‚Äî H)"]
        E2["Edge Node 2<br/>(Lathe ‚Äî M)"]
        E3["Edge Node 3<br/>(Press ‚Äî L)"]
        E4["Edge Node 4<br/>(Grinder ‚Äî H)"]
        E5["Edge Node 5<br/>(Drill ‚Äî M)"]
    end

    subgraph Fog["‚òÅÔ∏è Fog Layer"]
        K["Apache Kafka<br/>KRaft Mode"]
        ML["ML Service<br/>XGBoost / Random Forest"]
        WS["WebSocket Server<br/>Port 8080"]
        HTTP["Express Health API<br/>Port 3000"]
    end

    subgraph Dashboard["üìä Dashboard Layer"]
        UI["React Frontend<br/>Vite + Tailwind"]
    end

    E1 & E2 & E3 & E4 & E5 -->|"Produce ‚Üí machine-data"| K
    K -->|"Consume"| ML
    ML -->|"Produce ‚Üí prediction-data"| K
    K -->|"Consume"| WS
    WS -->|"WebSocket Push"| UI
    HTTP -.->|"Health / Stats"| UI

    style Edge fill:#1a1a2e,stroke:#e94560,color:#fff
    style Fog fill:#16213e,stroke:#0f3460,color:#fff
    style Dashboard fill:#0f3460,stroke:#533483,color:#fff
```

### Data Flow Pipeline

```mermaid
sequenceDiagram
    participant Edge as üîß Edge Node
    participant Kafka as üì® Kafka Broker
    participant ML as ü§ñ ML Service
    participant Backend as ‚öôÔ∏è Backend
    participant WS as üîå WebSocket
    participant UI as üìä Dashboard

    Edge->>Kafka: Publish sensor telemetry<br/>(topic: machine-data)
    Kafka->>ML: Consume raw readings
    ML->>ML: Feature engineering<br/>+ Model inference
    ML->>Kafka: Publish prediction<br/>(topic: prediction-data)
    Kafka->>Backend: Consume predictions
    Backend->>WS: Forward to WebSocket
    WS->>UI: Real-time push<br/>(JSON over WS)
    UI->>UI: Render charts,<br/>alerts & status
```

### Component Interaction

```mermaid
flowchart LR
    subgraph Telemetry["Sensor Telemetry"]
        direction TB
        T1["Air Temperature (K)"]
        T2["Process Temperature (K)"]
        T3["Rotational Speed (RPM)"]
        T4["Torque (Nm)"]
        T5["Tool Wear (min)"]
    end

    subgraph Detection["Anomaly Detection"]
        direction TB
        STAT["Statistical<br/>Boundary Check"]
        MLM["ML Model<br/>Prediction"]
    end

    subgraph Outcomes["Results"]
        direction TB
        OK["‚úÖ Normal Operation"]
        ALERT["üö® Alert Generated"]
    end

    Telemetry --> STAT
    STAT -->|"Within bounds"| MLM
    STAT -->|"Boundary violation"| ALERT
    MLM -->|"No anomaly"| OK
    MLM -->|"Anomaly detected"| ALERT
```

---

## Tech Stack

| Layer | Technology | Purpose |
|:------|:-----------|:--------|
| **Edge** | Node.js 22, TypeScript, KafkaJS | Sensor data simulation & Kafka publishing |
| **Kafka** | Confluent Kafka 7.6.0 (KRaft) | Event streaming, topic partitioning, consumer groups |
| **ML (Dockerized)** | Python 3.11, FastAPI, Confluent Kafka | Real-time Kafka consumer ‚Üí ML inference ‚Üí Kafka producer |
| **ML (Standalone)** | Python, Flask, scikit-learn, XGBoost | REST API for batch & single predictions |
| **Backend** | Node.js, Express, WebSocket (ws) | Kafka ‚Üí WebSocket bridge, health/stats endpoints |
| **Frontend** | React 19, Vite, Tailwind CSS, Recharts | Real-time dashboard, charts, alerts UI |
| **State** | Zustand | Frontend state management |
| **Animations** | Framer Motion | UI transitions & micro-animations |
| **Infra** | Docker, Docker Compose | One-command deployment of all services |

---

## Project Structure

```text
alerion-ai/
‚îÇ
‚îú‚îÄ‚îÄ alerion-backend/              # ‚öôÔ∏è Fog Layer (Node.js + Docker services)
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.ts              #    Backend entrypoint (Express + WS + Consumers)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kafka.ts          #    Kafka client factory & topic management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consumers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mlConsumer.ts     #    Kafka ‚Üí ML inference ‚Üí Kafka pipeline
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ predictionConsumer.ts  # Kafka ‚Üí WebSocket bridge
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ producers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ machineProducer.ts     # Edge ‚Üí Kafka publisher
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ edge/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ edgeSimulator.ts  #    Shared telemetry generator (Gaussian noise + drift)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ edgeNode1.ts      #    CNC Milling Machine (High capacity)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ edgeNode2.ts      #    Industrial Lathe (Medium)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ edgeNode3.ts      #    Hydraulic Press (Low)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ edgeNode4.ts      #    Surface Grinder (High)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ edgeNode5.ts      #    Precision Drill (Medium)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ websocket/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wsServer.ts       #    WebSocket broadcast server
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ machine.types.ts  #    Shared TypeScript interfaces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml/                   #    Mock ML consumer (fallback)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ wsTestClient.ts   #    WebSocket test client
‚îÇ   ‚îú‚îÄ‚îÄ ml-service/               # ü§ñ Dockerized FastAPI ML microservice
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py           #    FastAPI app + Kafka consumer/producer
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ predictor.py      #    ML model loader + inference
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model/                #    Trained model artifacts (.pkl)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml        # üê≥ Full stack orchestration
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                #    Backend container
‚îÇ   ‚îú‚îÄ‚îÄ .env.example              #    Environment template
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ AlerionAI/                    # üìä Dashboard (React + Vite)
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Landing.tsx       #    Landing page
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.tsx     #    Real-time monitoring dashboard
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Login.tsx         #    Authentication
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SignUp.tsx        #    Registration
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ About.tsx         #    About page
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Documentation.tsx #    Docs page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/           #    25 reusable UI components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ telemetryService.ts  # WebSocket client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store/                #    Zustand state stores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/                #    Custom React hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/                #    TypeScript types
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles/               #    Global styles
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ vite.config.ts
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ ml/                           # üß™ Standalone ML Training & API
‚îÇ   ‚îú‚îÄ‚îÄ app.py                    #    Flask REST API (/predict, /predict/batch)
‚îÇ   ‚îú‚îÄ‚îÄ training_notebook.ipynb   #    Jupyter notebook ‚Äî model training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ predictive_maintenance.csv#    Training dataset (10,000 records)
‚îÇ   ‚îú‚îÄ‚îÄ model_artifacts/          #    Saved models, scaler, label encoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ label_encoder.pkl
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ *.png                     #    Visualizations (confusion matrices, distributions)
‚îÇ
‚îú‚îÄ‚îÄ infrastructure/               # üèóÔ∏è Infrastructure configs
‚îÇ   ‚îî‚îÄ‚îÄ kafka/                    #    Kafka configuration files
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md                     #    ‚Üê You are here
```

---

## Prerequisites

| Tool | Version | Install |
|:-----|:--------|:--------|
| **Node.js** | ‚â• 22.x | [nodejs.org](https://nodejs.org/) |
| **npm** | ‚â• 10.x | Included with Node.js |
| **Docker** | ‚â• 24.x | [docker.com](https://docker.com/) |
| **Docker Compose** | ‚â• 2.20 | Included with Docker Desktop |
| **Python** | ‚â• 3.10 | [python.org](https://python.org/) *(only for standalone ML)* |

---

## Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/DevanshBehl/Alerion-AI.git
cd Alerion-AI
```

### 2. Start the Fog Layer (Docker)

This single command boots **Kafka**, the **Python ML service**, and the **Node.js backend**:

```bash
cd alerion-backend

# Copy the environment template
cp .env.example .env

# Start all services (Kafka + ML + Backend)
docker compose up -d
```

**Verify services are running:**

```bash
# Check container status
docker compose ps

# Expected output:
# alerion-kafka        Running   0.0.0.0:9092->9092/tcp
# alerion-ml-service   Running   0.0.0.0:8000->8000/tcp
# alerion-backend      Running   0.0.0.0:3000->3000/tcp, 0.0.0.0:8080->8080/tcp

# Check backend health
curl http://localhost:3000/health

# Check ML service health
curl http://localhost:8000/health
```

<details>
<summary>üìã <strong>Individual service commands</strong></summary>

```bash
# Start only Kafka
docker compose up kafka -d

# Start all services and follow logs
docker compose up -d && docker compose logs -f

# View ML service logs
docker compose logs -f ml-service

# Scale ML service to 3 replicas
docker compose up --scale ml-service=3 -d

# Stop all services
docker compose down

# Stop and remove volumes (clean reset)
docker compose down -v
```

</details>

### 3. Start Edge Nodes

Open a **new terminal** (keep Docker running):

```bash
cd alerion-backend

# Install dependencies (first time only)
npm install

# Option A: Start all 5 edge nodes simultaneously
npm run start:edges

# Option B: Start individual edge nodes
npm run start:edge1   # CNC Milling Machine (High capacity)
npm run start:edge2   # Industrial Lathe (Medium capacity)
npm run start:edge3   # Hydraulic Press (Low capacity)
npm run start:edge4   # Surface Grinder (High capacity)
npm run start:edge5   # Precision Drill (Medium capacity)
```

You should see telemetry being sent:

```text
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ALERION AI ‚Äî Edge Node Simulator          ‚ïë
‚ïë  Machine: MACHINE-001                      ‚ïë
‚ïë  Type:    H                                ‚ïë
‚ïë  Interval: 500ms                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

[MACHINE-001] ‚Üí Sent: temp=305.2K, rpm=1847, torque=42.3Nm
[MACHINE-001] ‚Üí Sent: temp=306.1K, rpm=1832, torque=43.1Nm
```

### 4. Start the Dashboard

Open **another terminal**:

```bash
cd AlerionAI

# Install dependencies (first time only)
npm install

# Start the development server
npm run dev
```

The dashboard will open at **http://localhost:5173** (Vite default).

> **‚ö° All three layers are now running!** Edge nodes ‚Üí Kafka ‚Üí ML ‚Üí WebSocket ‚Üí Dashboard

---

## Running Modes

### Single Machine (Development)

Everything runs on one machine. This is the default setup.

```mermaid
graph LR
    A["Terminal 1<br/>docker compose up -d"] --> B["Terminal 2<br/>npm run start:edges"]
    B --> C["Terminal 3<br/>npm run dev (frontend)"]

    style A fill:#1a1a2e,stroke:#e94560,color:#fff
    style B fill:#16213e,stroke:#0f3460,color:#fff
    style C fill:#0f3460,stroke:#533483,color:#fff
```

**Step-by-step:**

| Step | Terminal | Command | Purpose |
|:----:|:---------|:--------|:--------|
| 1 | Terminal 1 | `cd alerion-backend && docker compose up -d` | Start Kafka + ML + Backend |
| 2 | Terminal 2 | `cd alerion-backend && npm install && npm run start:edges` | Start edge nodes |
| 3 | Terminal 3 | `cd AlerionAI && npm install && npm run dev` | Start dashboard |
| 4 | Browser | Open `http://localhost:5173` | View real-time dashboard |

### Multi-Laptop (Distributed Demo)

For the full distributed experience across **3 physical machines** on the same network.

```mermaid
graph TB
    subgraph L1["üíª Laptop 1 ‚Äî Edge Layer"]
        Edge["5√ó Edge Nodes<br/>npm run start:edges"]
    end

    subgraph L2["üíª Laptop 2 ‚Äî Fog Layer"]
        Docker["Docker Compose<br/>(Kafka + ML + Backend)"]
    end

    subgraph L3["üíª Laptop 3 ‚Äî Dashboard"]
        React["React Frontend<br/>npm run dev"]
    end

    Edge -->|"Kafka TCP<br/>IP:9092"| Docker
    Docker -->|"WebSocket<br/>IP:8080"| React

    style L1 fill:#1a1a2e,stroke:#e94560,color:#fff
    style L2 fill:#16213e,stroke:#0f3460,color:#fff
    style L3 fill:#0f3460,stroke:#533483,color:#fff
```

**Step-by-step:**

#### Step 1 ‚Äî Find the Fog Laptop IP

On **Laptop 2** (Fog), run:

```bash
# macOS
ipconfig getifaddr en0

# Linux
hostname -I | awk '{print $1}'

# Windows
ipconfig | findstr "IPv4"
```

> Example output: `192.168.1.45`

#### Step 2 ‚Äî Start Fog Layer (Laptop 2)

```bash
cd alerion-backend

# Update Kafka advertised listeners for external access
# Edit docker-compose.yml ‚Üí KAFKA_ADVERTISED_LISTENERS:
#   EXTERNAL://192.168.1.45:9092  (replace localhost)

docker compose up -d
```

#### Step 3 ‚Äî Start Edge Nodes (Laptop 1)

```bash
cd alerion-backend

# Update .env to point to Fog laptop
echo "KAFKA_BROKERS=192.168.1.45:9092" > .env

npm install
npm run start:edges
```

#### Step 4 ‚Äî Start Dashboard (Laptop 3)

```bash
cd AlerionAI

# Update WebSocket URL in the telemetry service to:
# ws://192.168.1.45:8080

npm install
npm run dev
```

---

## ML Model Details

### Training Pipeline

The model is trained on the [Predictive Maintenance Dataset](https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset) (10,000 records) with the following pipeline:

```mermaid
flowchart LR
    A["Raw CSV<br/>10K records"] --> B["Feature<br/>Engineering"]
    B --> C["SMOTE<br/>Oversampling"]
    C --> D["Train/Test<br/>Split"]
    D --> E["Model Training<br/>XGBoost + RF"]
    E --> F["Model<br/>Evaluation"]
    F --> G["Export<br/>model.pkl"]

    style A fill:#1a1a2e,stroke:#e94560,color:#fff
    style G fill:#0f3460,stroke:#533483,color:#fff
```

### Features Used

| Feature | Description | Range |
|:--------|:------------|:------|
| `Air temperature [K]` | Ambient air temperature | 290‚Äì320 K |
| `Process temperature [K]` | Process operating temperature | 300‚Äì340 K |
| `Rotational speed [rpm]` | Spindle RPM | 1,000‚Äì3,000 |
| `Torque [Nm]` | Torque applied | 10‚Äì80 Nm |
| `Tool wear [min]` | Cumulative tool usage | 0‚Äì250 min |
| `type_encoded` | Machine type (L=0, M=1, H=2) | 0‚Äì2 |
| `temp_diff` | Process ‚àí Air temperature | Derived |
| `power_W` | Torque √ó angular velocity | Derived |
| `torque_x_wear` | Torque √ó tool wear | Derived |
| `rpm_per_torque` | RPM / Torque ratio | Derived |

### Failure Classes

| Class | Severity | Description |
|:------|:---------|:------------|
| No Failure | ‚Äî | Normal operation |
| Heat Dissipation Failure | üî¥ High | Overheating condition |
| Power Failure | üî¥ High | Electrical/power anomaly |
| Overstrain Failure | üî¥ High | Mechanical overstress |
| Tool Wear Failure | üü° Medium | Gradual tool degradation |
| Random Failures | üü° Medium | Unpredictable failure events |

### Running the Standalone ML API

```bash
cd ml

# Create virtual environment
python -m venv .venv
source .venv/bin/activate    # macOS/Linux
# .venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt

# Start the Flask API
python app.py
# ‚Üí API running at http://localhost:5000
```

**Test a prediction:**

```bash
curl -X POST http://localhost:5000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "air_temperature": 298.1,
    "process_temperature": 308.6,
    "rotational_speed": 1551,
    "torque": 42.8,
    "tool_wear": 0,
    "machine_type": "M"
  }'
```

---

## API Reference

### Backend (Express ‚Äî Port 3000)

| Method | Endpoint | Description |
|:-------|:---------|:------------|
| `GET` | `/health` | Service health check (uptime, status) |
| `GET` | `/stats` | WebSocket stats (connected clients, messages) |
| `GET` | `/ready` | Readiness probe for orchestrators |

### ML Service ‚Äî Flask (Port 5000)

| Method | Endpoint | Description |
|:-------|:---------|:------------|
| `GET` | `/health` | Model info & service status |
| `GET` | `/classes` | List all failure classes |
| `GET` | `/metadata` | Full model metadata (features, metrics) |
| `POST` | `/predict` | Single reading prediction |
| `POST` | `/predict/batch` | Batch predictions (max 100 readings) |

### ML Service ‚Äî FastAPI / Docker (Port 8000)

The Dockerized ML service consumes directly from Kafka (`machine-data` topic), runs inference, and publishes results to the `prediction-data` topic. It also exposes a health endpoint.

### WebSocket (Port 8080)

Clients connect to `ws://localhost:8080` and receive JSON messages:

```json
{
  "type": "prediction",
  "payload": {
    "machine_id": "MACHINE-001",
    "machine_type": "H",
    "air_temperature": 305.2,
    "process_temperature": 312.4,
    "rotational_speed": 1847,
    "torque": 42.3,
    "tool_wear": 128,
    "prediction": 0,
    "confidence": 0.94,
    "anomalyScore": 0.12,
    "failure_type": "No Failure",
    "processed_at": "2026-02-19T01:10:00.000Z"
  },
  "timestamp": "2026-02-19T01:10:00.000Z"
}
```

Message types: `prediction` | `alert` | `heartbeat` | `system`

---

## Kafka Topics

| Topic | Partitions | Producer | Consumer | Description |
|:------|:-----------|:---------|:---------|:------------|
| `machine-data` | 5 | Edge Nodes | ML Service | Raw sensor telemetry |
| `prediction-data` | 5 | ML Service | Backend | ML prediction results |

Topics are auto-created by Kafka (configured via `KAFKA_AUTO_CREATE_TOPICS_ENABLE=true`), or you can create them manually:

```bash
# Create topics manually
docker exec -it alerion-kafka kafka-topics \
  --create --topic machine-data \
  --bootstrap-server localhost:9092 \
  --partitions 5 --replication-factor 1

docker exec -it alerion-kafka kafka-topics \
  --create --topic prediction-data \
  --bootstrap-server localhost:9092 \
  --partitions 5 --replication-factor 1

# List all topics
docker exec -it alerion-kafka kafka-topics \
  --list --bootstrap-server localhost:9092

# Consume messages from a topic (debug)
docker exec -it alerion-kafka kafka-console-consumer \
  --topic machine-data \
  --bootstrap-server localhost:9092 \
  --from-beginning
```

---

## Environment Variables

### Backend (`alerion-backend/.env`)

| Variable | Default | Description |
|:---------|:--------|:------------|
| `KAFKA_BROKERS` | `localhost:9092` | Kafka bootstrap servers |
| `KAFKA_CLIENT_ID` | `alerion-backend` | Kafka client identifier |
| `WS_PORT` | `8080` | WebSocket server port |
| `HTTP_PORT` | `3000` | Express health server port |
| `ML_SERVICE_URL` | `http://localhost:8000` | Python ML service URL |
| `USE_MOCK_ML` | `true` | Use TypeScript mock ML (set `false` for Python service) |
| `MACHINE_DATA_TOPIC` | `machine-data` | Topic for raw telemetry |
| `PREDICTION_DATA_TOPIC` | `prediction-data` | Topic for ML predictions |

---

## Scalability & Performance

### Current Capabilities

| Metric | Value |
|:-------|:------|
| Edge nodes (demo) | 5 concurrent simulators |
| Message rate (demo) | ~10 msg/sec (2/sec per node) |
| Max throughput | 10,000+ msg/sec (hardware dependent) |
| Kafka partitions | 5 per topic |
| WebSocket clients | Unlimited (broadcast) |
| ML replicas | Scalable via `docker compose --scale` |

### Scalability Architecture

```mermaid
graph TB
    subgraph Scale["Horizontal Scaling Points"]
        A["üîß Edge Nodes<br/>Add more node configs"]
        B["üì® Kafka<br/>Multi-broker cluster"]
        C["ü§ñ ML Workers<br/>docker compose scale"]
        D["üîå WebSocket<br/>Load-balanced instances"]
        E["‚òÅÔ∏è Cloud<br/>AWS / GCP / Azure"]
    end

    A --> B --> C --> D --> E

    style Scale fill:#1a1a2e,stroke:#e94560,color:#fff
```

---

## Future Roadmap

- [ ] Kubernetes deployment manifests (Helm charts)
- [ ] Multi-broker Kafka cluster for fault tolerance
- [ ] Redis caching for prediction results
- [ ] Alert escalation via email / SMS / Slack
- [ ] Edge AI preprocessing (TensorFlow Lite)
- [ ] Time-series database integration (InfluxDB / TimescaleDB)
- [ ] Grafana monitoring dashboards
- [ ] OAuth2 / SSO authentication
- [ ] CI/CD pipeline (GitHub Actions)

---

## Contributing

1. **Fork** the repository
2. Create a feature branch: `git checkout -b feature/my-feature`
3. Commit changes: `git commit -m "feat: add my feature"`
4. Push to the branch: `git push origin feature/my-feature`
5. Open a **Pull Request**

---

## License

This project is developed by the **Alerion AI Team** for distributed systems & AI engineering research.

---

<div align="center">

**Built with ‚ù§Ô∏è by [Devansh Behl](https://github.com/DevanshBehl)**

*Distributed Systems & AI Engineering*

</div>
]]>